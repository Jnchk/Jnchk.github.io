<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>How to deploy ceph with ceph-deploy - KillMeBaby</title>
  <meta property="og:title" content="How to deploy ceph with ceph-deploy - KillMeBaby" />
  <meta name="twitter:title" content="How to deploy ceph with ceph-deploy - KillMeBaby" />
  <meta name="description" content="本文介绍如何通过ceph-deploy部署ceph集群。">
  <meta property="og:description" content="本文介绍如何通过ceph-deploy部署ceph集群。">
  <meta name="twitter:description" content="本文介绍如何通过ceph-deploy部署ceph集群。">
  <meta name="author" content="Jnchk"/>
  <link rel='icon' type='image/x-icon' href="https://killmebaby.cc/img/favicon.ico" />
  <meta property="og:site_name" content="KillMeBaby" />
  <meta property="og:url" content="https://killmebaby.cc/posts/2018-02-24-ceph-deployment/" />
  <meta property="og:type" content="article" />
  <meta name="twitter:card" content="summary" />
  <meta name="generator" content="Hugo 0.120.4">
  <link rel="stylesheet" href="/css/style.css" media="all" />
  <link rel="stylesheet" href="/css/style-dark.css" media="all and (prefers-color-scheme: dark)" />

  <link rel="stylesheet" href="/css/syntax.css" media="all" />
  <link rel="stylesheet" href="/css/custom.css" media="all" />

  <script src="/js/script.js"></script>
  <script src="/js/custom.js"></script>
  <script defer src="/js/fontawesome.js"></script>
</head>

<body>

<header class="site-header">
  <nav class="site-navi">
    <h1 class="site-title"><a href="/">KillMeBaby</a></h1>
    <ul class="site-navi-items">
      <li class="site-navi-item-categories"><a href="/categories/" title="Categories">Categories</a></li>
      <li class="site-navi-item-tags"><a href="/tags/" title="Tags">Tags</a></li>
      <li class="site-navi-item-archives"><a href="/archives/" title="Archives">Archives</a></li>
      <li class="site-navi-item-about"><a href="/about/" title="About">About</a></li>
    </ul>
  </nav>
</header>
<hr class="site-header-bottom">

  <div class="main" role="main">
    <article class="article">
      
      
      <h1 class="article-title">How to deploy ceph with ceph-deploy</h1>
      
      <hr class="article-title-bottom">
      <ul class="article-meta">
        <li class="article-meta-date"><time>February 24, 2018</time></li>
        <li class="article-meta-categories">
          <a href="/categories/software-deployment/">
            <i class="fas fa-folder"></i>
            software-deployment
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/ceph/">
            <i class="fas fa-tag"></i>
            ceph
          </a>&nbsp;
        </li>
        <li class="article-meta-tags">
          <a href="/tags/deployment/">
            <i class="fas fa-tag"></i>
            deployment
          </a>&nbsp;
        </li>
      </ul>
      
<aside class="toc">
  <nav id="TableOfContents">
  <ul>
    <li><a href="#环境描述">环境描述</a></li>
    <li><a href="#安装ceph集群">安装ceph集群</a></li>
    <li><a href="#通过ceph-rbd挂载磁盘">通过ceph-rbd挂载磁盘</a></li>
    <li><a href="#部署cephfs">部署cephfs</a></li>
    <li><a href="#通过cephfs挂载磁盘到本地">通过cephfs挂载磁盘到本地</a></li>
    <li><a href="#部署过程中可能遇到的问题">部署过程中可能遇到的问题</a></li>
  </ul>
</nav>
</aside>
      <p>本文介绍如何通过ceph-deploy部署ceph集群。</p>
<h1 id="环境描述">环境描述</h1>
<p>准备三台主机，ceph01，ceph01，ceph02。并且这三台主机都需要准备一个额外的磁盘，如本例中的/dev/sdb。同时，我们还需要为/dev/sdb格式化，并设置免密登录。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ssh-keygen
</span></span><span class="line"><span class="cl">$ ssh-copy-id root@ceph01
</span></span><span class="line"><span class="cl">$ ssh-copy-id root@ceph02
</span></span><span class="line"><span class="cl">$ ssh-copy-id root@ceph03
</span></span><span class="line"><span class="cl">$ mkfs.xfs /dev/sdb  <span class="c1"># on three nodes</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">$ apt install ceph-deploy
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="安装ceph集群">安装ceph集群</h1>
<p>准备好环境即可通过ceph-deploy部署ceph集群。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ mkdir my-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd</span> my-cluster
</span></span><span class="line"><span class="cl">$ ceph-deploy new ceph01                   <span class="c1"># create cluster</span>
</span></span><span class="line"><span class="cl">$ ceph-deploy install ceph01 ceph02 ceph03 <span class="c1"># install ceph package</span>
</span></span><span class="line"><span class="cl">$ ceph-deploy mon create-initial           <span class="c1"># deploy the initial monitor and gather the keys</span>
</span></span><span class="line"><span class="cl">$ ceph-deploy admin ceph01 ceph02 ceph03   <span class="c1"># copy configuration files to nodes</span>
</span></span><span class="line"><span class="cl">$ ceph-deploy osd create ceph01:sdb ceph02:sdb ceph03:sdb <span class="c1"># create OSDs</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>按照上面的部署过程，我们会得到一个monitor+三个osd的ceph集群，并且所有部署操作都是在ceph01上进行的，之后对ceph集群的改动也要在此机器上做。</p>
<p>而因为monitor也部署在ceph01这台机器上，所以外部的请求也要发送到该机器上（比如下面挂载cephfs的例子）。</p>
<p>之后验证集群状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph health
</span></span><span class="line"><span class="cl">HEALTH_OK
</span></span><span class="line"><span class="cl">$ ceph -s
</span></span><span class="line"><span class="cl">    cluster 2e19ebb4-d5a1-4916-a2b8-340a9ccf220a
</span></span><span class="line"><span class="cl">     health HEALTH_OK
</span></span><span class="line"><span class="cl">     monmap e1: <span class="m">1</span> mons at <span class="o">{</span><span class="nv">ceph01</span><span class="o">=</span>192.168.73.128:6789/0<span class="o">}</span>
</span></span><span class="line"><span class="cl">            election epoch 3, quorum <span class="m">0</span> ceph01
</span></span><span class="line"><span class="cl">     osdmap e14: <span class="m">3</span> osds: <span class="m">3</span> up, <span class="m">3</span> in
</span></span><span class="line"><span class="cl">            flags sortbitwise,require_jewel_osds
</span></span><span class="line"><span class="cl">      pgmap v33: <span class="m">64</span> pgs, <span class="m">1</span> pools, <span class="m">0</span> bytes data, <span class="m">0</span> objects
</span></span><span class="line"><span class="cl">            <span class="m">100</span> MB used, <span class="m">45946</span> MB / <span class="m">46046</span> MB avail
</span></span><span class="line"><span class="cl">                  <span class="m">64</span> active+clean
</span></span><span class="line"><span class="cl">$ ceph df
</span></span><span class="line"><span class="cl">GLOBAL:
</span></span><span class="line"><span class="cl">    SIZE       AVAIL      RAW USED     %RAW USED
</span></span><span class="line"><span class="cl">    46046M     45946M         100M          0.22
</span></span><span class="line"><span class="cl">POOLS:
</span></span><span class="line"><span class="cl">    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS
</span></span><span class="line"><span class="cl">    rbd      <span class="m">0</span>         <span class="m">0</span>         <span class="m">0</span>        15315M           <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="通过ceph-rbd挂载磁盘">通过ceph-rbd挂载磁盘</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 创建pool，也可以使用默认的pool rdb</span>
</span></span><span class="line"><span class="cl">$ ceph osd pool create first-pool <span class="m">8</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 创建rbd</span>
</span></span><span class="line"><span class="cl">$ rbd create foo --size <span class="m">128</span> --image-feature layering --pool first-pool
</span></span><span class="line"><span class="cl">$ rbd map foo --pool first-pool
</span></span><span class="line"><span class="cl">/dev/rbd0
</span></span><span class="line"><span class="cl">$ mkfs -t ext4 /dev/rbd0
</span></span><span class="line"><span class="cl">$ mount -t ext4 /dev/rbd0 /root/test
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="部署cephfs">部署cephfs</h1>
<p>为了使用cephfs，我们需要先部署mds，即metadata server。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph-deploy mds create ceph01
</span></span></code></pre></td></tr></table>
</div>
</div><p>之后可以创建相关的pool与filesystem</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph osd pool create cephfs_data <span class="m">8</span>
</span></span><span class="line"><span class="cl">$ ceph osd pool create cephfs_metadata <span class="m">8</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">$ ceph fs new cephfs cephfs_metadata cephfs_data
</span></span></code></pre></td></tr></table>
</div>
</div><p>通过mds的状态验证创建是否成功</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph mds stat
</span></span><span class="line"><span class="cl">e7: 1/1/1 up <span class="o">{</span><span class="nv">0</span><span class="o">=</span><span class="nv">ceph01</span><span class="o">=</span>up:active<span class="o">}</span>    <span class="c1"># 即为正常</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph mds stat
</span></span><span class="line"><span class="cl">e2: 0/0/1 up     <span class="c1"># 即为不正常，需要定位问题</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="通过cephfs挂载磁盘到本地">通过cephfs挂载磁盘到本地</h1>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 在部署主机上拿到admin用户的key文件，否则直接mount会遇到 libceph: error -22 on auth protocol 2 init 类似的错误</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 比如我们拿到key： AQBpMXBat2emEhAAXm6p3Xq0uGtw1qWq9EbEcA==</span>
</span></span><span class="line"><span class="cl">$ mount -t ceph 192.168.73.128:6789:/ /root/test_cephfs/ -o <span class="nv">name</span><span class="o">=</span>admin,secret<span class="o">=</span><span class="nv">AQBpMXBat2emEhAAXm6p3Xq0uGtw1qWq9EbEcA</span><span class="o">==</span>
</span></span><span class="line"><span class="cl">$ df -h
</span></span><span class="line"><span class="cl">Filesystem                        Size  Used Avail Use% Mounted on
</span></span><span class="line"><span class="cl">udev                              468M     <span class="m">0</span>  468M   0% /dev
</span></span><span class="line"><span class="cl">tmpfs                              98M  5.8M   92M   6% /run
</span></span><span class="line"><span class="cl">/dev/mapper/wille--vg-root         27G  2.1G   24G   8% /
</span></span><span class="line"><span class="cl">tmpfs                             488M     <span class="m">0</span>  488M   0% /dev/shm
</span></span><span class="line"><span class="cl">tmpfs                             5.0M     <span class="m">0</span>  5.0M   0% /run/lock
</span></span><span class="line"><span class="cl">tmpfs                             488M     <span class="m">0</span>  488M   0% /sys/fs/cgroup
</span></span><span class="line"><span class="cl">/dev/sda1                         472M  106M  342M  24% /boot
</span></span><span class="line"><span class="cl">/dev/mapper/wille--vg-root--home   20G   45M   19G   1% /home
</span></span><span class="line"><span class="cl">tmpfs                              98M     <span class="m">0</span>   98M   0% /run/user/0
</span></span><span class="line"><span class="cl">/dev/rbd0                         120M  1.6M  110M   2% /root/test
</span></span><span class="line"><span class="cl">192.168.73.128:6789:/              45G  120M   45G   1% /root/test_cephfs
</span></span><span class="line"><span class="cl"><span class="c1"># 然后可以看到cephfs_metadata这个pool中多出了一部分数据</span>
</span></span><span class="line"><span class="cl">$ ceph df
</span></span><span class="line"><span class="cl">GLOBAL:
</span></span><span class="line"><span class="cl">    SIZE       AVAIL      RAW USED     %RAW USED
</span></span><span class="line"><span class="cl">    46046M     45926M         120M          0.26
</span></span><span class="line"><span class="cl">POOLS:
</span></span><span class="line"><span class="cl">    NAME                ID     USED      %USED     MAX AVAIL     OBJECTS
</span></span><span class="line"><span class="cl">    rbd                 <span class="m">0</span>      8820k      0.06        15307M          <span class="m">15</span>
</span></span><span class="line"><span class="cl">    first-pool          <span class="m">1</span>          <span class="m">0</span>         <span class="m">0</span>        15307M           <span class="m">0</span>
</span></span><span class="line"><span class="cl">    cephfs_data         <span class="m">2</span>          <span class="m">0</span>         <span class="m">0</span>        15307M           <span class="m">0</span>
</span></span><span class="line"><span class="cl">    cephfs_metadata     <span class="m">3</span>       <span class="m">2545</span>         <span class="m">0</span>        15307M          <span class="m">20</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 当我们把一个39M的文件copy到挂载后的目录（/root/test_cephfs）中之后，可以看到cephfs_data中也多出了对应的数据</span>
</span></span><span class="line"><span class="cl">$ ceph df
</span></span><span class="line"><span class="cl">GLOBAL:
</span></span><span class="line"><span class="cl">    SIZE       AVAIL      RAW USED     %RAW USED
</span></span><span class="line"><span class="cl">    46046M     45814M         232M          0.51
</span></span><span class="line"><span class="cl">POOLS:
</span></span><span class="line"><span class="cl">    NAME                ID     USED       %USED     MAX AVAIL     OBJECTS
</span></span><span class="line"><span class="cl">    rbd                 <span class="m">0</span>       8820k      0.06        15270M          <span class="m">15</span>
</span></span><span class="line"><span class="cl">    first-pool          <span class="m">1</span>           <span class="m">0</span>         <span class="m">0</span>        15270M           <span class="m">0</span>
</span></span><span class="line"><span class="cl">    cephfs_data         <span class="m">2</span>      38977k      0.25        15270M          <span class="m">10</span>
</span></span><span class="line"><span class="cl">    cephfs_metadata     <span class="m">3</span>       <span class="m">37033</span>         <span class="m">0</span>        15270M          <span class="m">20</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h1 id="部署过程中可能遇到的问题">部署过程中可能遇到的问题</h1>
<p>遇到ceph health提示HEALTH_WARN</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 当我们执行ceph -s时可能遇到下面的错误</span>
</span></span><span class="line"><span class="cl">$ ceph -s
</span></span><span class="line"><span class="cl">cluster b3609cba-0b6d-4311-8aa3-6968c0e66f5e
</span></span><span class="line"><span class="cl"> health HEALTH_WARN
</span></span><span class="line"><span class="cl">        <span class="m">64</span> pgs degraded
</span></span><span class="line"><span class="cl">        <span class="m">64</span> pgs stuck degraded
</span></span><span class="line"><span class="cl">        <span class="m">64</span> pgs stuck unclean
</span></span><span class="line"><span class="cl">        <span class="m">64</span> pgs stuck undersized
</span></span><span class="line"><span class="cl">        <span class="m">64</span> pgs undersized
</span></span><span class="line"><span class="cl"> monmap e1: <span class="m">1</span> mons at <span class="o">{</span><span class="nv">0</span><span class="o">=</span>10.11.108.188:6789/0<span class="o">}</span>
</span></span><span class="line"><span class="cl">        election epoch 3, quorum <span class="m">0</span> <span class="m">0</span>
</span></span><span class="line"><span class="cl"> osdmap e15: <span class="m">2</span> osds: <span class="m">2</span> up, <span class="m">2</span> in
</span></span><span class="line"><span class="cl">        flags sortbitwise,require_jewel_osds
</span></span><span class="line"><span class="cl">  pgmap v36: <span class="m">64</span> pgs, <span class="m">1</span> pools, <span class="m">0</span> bytes data, <span class="m">0</span> objects
</span></span><span class="line"><span class="cl">        <span class="m">69172</span> kB used, <span class="m">3338</span> GB / <span class="m">3338</span> GB avail
</span></span><span class="line"><span class="cl">              _64 active+undersized+degraded
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 这一般是我们所使用的/dev/sdb被格式化为了ext4的文件系统</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 在我的实验环境里，通过ceph.conf设置一些全局参数没能解决该问题</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 将/dev/sdb格式化为xfs并重新部署ceph之后即解决了</span>
</span></span></code></pre></td></tr></table>
</div>
</div>
    </article>

    


    <ul class="pager article-pager">
      <li class="pager-newer">
          <a href="/posts/2018-08-05-kubernetes-deployment-with-kubespray/" data-toggle="tooltip" data-placement="top" title="How to deploy Kubernetes with Kubespray">&lt; Newer</a>
      </li>
      <li class="pager-older">
        <a href="/posts/2018-02-06-redis-cluster-deployment/" data-toggle="tooltip" data-placement="top" title="How to deploy redis cluster">Older &gt;</a>
      </li>
    </ul>
  </div>


<div class="site-footer">
  <div class="copyright">&copy; Copyright 2018-2023 Jnchk</div>
  <ul class="site-footer-items">
  </ul>
  <div class="powerdby">
    Powered by <a href="https://gohugo.io/">Hugo</a> and <a href="https://github.com/taikii/whiteplain">Whiteplain</a>
  </div>
</div>


</body>
</html>
